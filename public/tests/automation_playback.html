<!DOCTYPE html>
<meta charset="utf-8">
<html>
    <head>
        <title>Automation Playback</title>
        <link rel="shortcut icon" href="../images/icon.png" type="image/gif">
        <script src="../js/prong.js"></script>
        <link rel='stylesheet' href='testPageMixer.css'></link>
        <link rel='stylesheet' href='testPage.css'></link>
        <script src="qadi-min.js"></script>
        <link rel="stylesheet" type="text/css" href="qadi.css"/>

    </head>
    <body>

        <iframe src="./"></iframe>
        <div id="main">

        Automation Playback<br />
        <ul id="qadi">
            <li>You see two tracks, each with automation lines</li>
            <li>When yo click to set the playhead, the volumes change to
            match the automation value at that time</li>
            <li>When you play the audio, the faders move to reflect the
            changes in the automation curve</li>
            <li>When you play the audio, you can hear the changes in audio
            volume corresponding to the changes in the automation curve</li>
            <li>If you set a loop and play it, then when it loops the volume is 
            correct (both visually and audibly)</li>
        </ul>
        <span>
        
        <br />
        </span>
        <div id="transport"></div>
        <div id="sequence"></div>
        <br />
        <div id="mixer"></div>
        </div>
        <script>

var rootUrl = 'https://dl.dropboxusercontent.com/u/5613860/audio/nino_rota/'

rootUrl = '/audio/'

var tracks = [
    {
        'type' : 'audio',
        'src' : rootUrl + 'sax1.mp3',
        'automation' : {
            'volume' : {
                'interpolate' : 'linear',
                'points' : [[0, 100],[2, 50],[3, 20],[10, 100],[11, 80]]
            }
        }
    },
    {
        'type' : 'audio',
        'src' : rootUrl + 'sax2.mp3',
        'automation' : {
            'volume' : {
                'interpolate' : 'step-after',
                'points' : [[0, 45],[2, 50],[3, 45],[4, 40],[5, 35]]
            }
        }
    },
    {
        'type' : 'audio',
        'src' : rootUrl + 'flute.mp3',
        'automation' : {
            'volume' : {
                'interpolate' : 'step-after',
                'points' : [[0, 30],[2, 50],[3, 60],[4, 70],[5, 80]]
            }
        }
    },
    {
        'type' : 'audio',
        'src' : rootUrl + 'tuba.mp3',
        'automation' : {
            'volume' : {
                'interpolate' : 'step-after',
                'points' : [[0, 100],[2, 20],[4, 80],[5, 30],[9, 10]]
            }
        }
    }
]

tracks = prong.omniscience.makeWatchable(tracks)

var d3 = prong.d3;
var x = d3.scale.linear().domain([0, 15]).range([0, 800]);

var sequence = prong.sequence()
    .x(x)
    .tracks(tracks)
    .trackHeight(40)
    .canSelectLoop(true)
    .audioOut(prong.audioContext().destination)
    .historyKey('s');

var transport = prong.transport().sequence(sequence);

var mixer = prong.mixer().sequence(sequence);

d3.select('#sequence').call(sequence);
d3.select('#transport').call(transport);
d3.select('#mixer').call(mixer);

        </script>
    </body>
</html>